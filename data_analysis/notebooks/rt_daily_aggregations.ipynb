{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24548ef1",
   "metadata": {},
   "source": [
    "Notebook updated 7/17/22 -- initial data analysis to aggregate RT data to hourly totals\n",
    "\n",
    "*This notebook will not work without access to the private S3 bucket; work is planned to split some of this notebook out to automated scripts to create the daily CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a67e21f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import pandas as pd\n",
    "import pendulum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d840608",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = '2022-07-17'\n",
    "end_date = '2022-08-07'\n",
    "\n",
    "date_range = [d for d in pendulum.period(pendulum.from_format(start_date, 'YYYY-MM-DD'), pendulum.from_format(end_date, 'YYYY-MM-DD')).range('days')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff74c212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get objects from S3\n",
    "# this requires being locally authenticated: https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-files.html\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "bucket = s3.Bucket('chn-ghost-buses-private')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9043cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 2022-07-17 at 2022-08-08 21:52:03\n",
      "------- loading data at 2022-08-08 21:52:03\n",
      "------- parsing data at 2022-08-08 21:52:33\n",
      "------- saving data at 2022-08-08 21:52:39\n",
      "Processing 2022-07-18 at 2022-08-08 21:52:46\n",
      "------- loading data at 2022-08-08 21:52:47\n",
      "------- parsing data at 2022-08-08 21:53:20\n",
      "------- saving data at 2022-08-08 21:53:29\n",
      "Processing 2022-07-19 at 2022-08-08 21:53:42\n",
      "------- loading data at 2022-08-08 21:53:42\n",
      "------- parsing data at 2022-08-08 21:54:17\n",
      "------- saving data at 2022-08-08 21:54:27\n",
      "Processing 2022-07-20 at 2022-08-08 21:54:39\n",
      "------- loading data at 2022-08-08 21:54:39\n",
      "------- parsing data at 2022-08-08 21:55:13\n",
      "------- saving data at 2022-08-08 21:55:22\n",
      "Processing 2022-07-21 at 2022-08-08 21:55:35\n",
      "------- loading data at 2022-08-08 21:55:35\n",
      "------- parsing data at 2022-08-08 21:56:08\n",
      "------- saving data at 2022-08-08 21:56:18\n",
      "Processing 2022-07-22 at 2022-08-08 21:56:32\n",
      "------- loading data at 2022-08-08 21:56:32\n",
      "------- parsing data at 2022-08-08 21:57:09\n",
      "------- saving data at 2022-08-08 21:57:18\n",
      "Processing 2022-07-23 at 2022-08-08 21:57:31\n",
      "------- loading data at 2022-08-08 21:57:31\n",
      "------- parsing data at 2022-08-08 21:58:02\n",
      "------- saving data at 2022-08-08 21:58:10\n",
      "Processing 2022-07-24 at 2022-08-08 21:58:19\n",
      "------- loading data at 2022-08-08 21:58:19\n",
      "------- parsing data at 2022-08-08 21:58:49\n",
      "------- saving data at 2022-08-08 21:58:56\n",
      "Processing 2022-07-25 at 2022-08-08 21:59:03\n",
      "------- loading data at 2022-08-08 21:59:03\n",
      "------- parsing data at 2022-08-08 21:59:37\n",
      "------- saving data at 2022-08-08 21:59:47\n",
      "Processing 2022-07-26 at 2022-08-08 22:00:00\n",
      "------- loading data at 2022-08-08 22:00:00\n",
      "------- parsing data at 2022-08-08 22:00:35\n",
      "------- saving data at 2022-08-08 22:00:44\n",
      "Processing 2022-07-27 at 2022-08-08 22:00:58\n",
      "------- loading data at 2022-08-08 22:00:58\n",
      "------- parsing data at 2022-08-08 22:01:33\n",
      "------- saving data at 2022-08-08 22:01:42\n",
      "Processing 2022-07-28 at 2022-08-08 22:01:55\n",
      "------- loading data at 2022-08-08 22:01:55\n",
      "------- parsing data at 2022-08-08 22:02:33\n",
      "------- saving data at 2022-08-08 22:02:41\n",
      "Processing 2022-07-29 at 2022-08-08 22:02:57\n",
      "------- loading data at 2022-08-08 22:02:57\n",
      "------- parsing data at 2022-08-08 22:03:32\n",
      "------- saving data at 2022-08-08 22:03:41\n",
      "Processing 2022-07-30 at 2022-08-08 22:03:53\n",
      "------- loading data at 2022-08-08 22:03:53\n",
      "------- parsing data at 2022-08-08 22:04:26\n",
      "------- saving data at 2022-08-08 22:04:33\n",
      "Processing 2022-07-31 at 2022-08-08 22:04:42\n",
      "------- loading data at 2022-08-08 22:04:42\n",
      "------- parsing data at 2022-08-08 22:05:15\n",
      "------- saving data at 2022-08-08 22:05:22\n",
      "Processing 2022-08-01 at 2022-08-08 22:05:29\n",
      "------- loading data at 2022-08-08 22:05:29\n",
      "------- parsing data at 2022-08-08 22:06:03\n",
      "------- saving data at 2022-08-08 22:06:12\n",
      "Processing 2022-08-02 at 2022-08-08 22:06:24\n",
      "------- loading data at 2022-08-08 22:06:24\n",
      "------- parsing data at 2022-08-08 22:07:00\n",
      "------- saving data at 2022-08-08 22:07:09\n",
      "Processing 2022-08-03 at 2022-08-08 22:07:22\n",
      "------- loading data at 2022-08-08 22:07:22\n",
      "------- parsing data at 2022-08-08 22:07:58\n",
      "------- saving data at 2022-08-08 22:08:08\n",
      "Processing 2022-08-04 at 2022-08-08 22:08:20\n",
      "------- loading data at 2022-08-08 22:08:20\n",
      "------- parsing data at 2022-08-08 22:08:56\n",
      "------- saving data at 2022-08-08 22:09:06\n",
      "Processing 2022-08-05 at 2022-08-08 22:09:19\n",
      "------- loading data at 2022-08-08 22:09:19\n",
      "------- parsing data at 2022-08-08 22:09:55\n",
      "------- saving data at 2022-08-08 22:10:07\n",
      "Processing 2022-08-06 at 2022-08-08 22:10:20\n",
      "------- loading data at 2022-08-08 22:10:20\n",
      "------- parsing data at 2022-08-08 22:10:54\n",
      "------- saving data at 2022-08-08 22:11:01\n",
      "Processing 2022-08-07 at 2022-08-08 22:11:09\n",
      "------- loading data at 2022-08-08 22:11:09\n",
      "------- parsing data at 2022-08-08 22:11:41\n",
      "------- saving data at 2022-08-08 22:11:48\n"
     ]
    }
   ],
   "source": [
    "for day in date_range:\n",
    "    date_str = day.to_date_string()\n",
    "    print(f\"Processing {date_str} at {pendulum.now().to_datetime_string()}\")\n",
    "    objects = bucket.objects.filter(Prefix = f'bus_data/{date_str}')\n",
    "    \n",
    "    print(f\"------- loading data at {pendulum.now().to_datetime_string()}\")\n",
    "    \n",
    "    # load data\n",
    "    data_dict = {}\n",
    "\n",
    "    for obj in objects:\n",
    "        # print(f\"loading {obj}\")\n",
    "        obj_name = obj.key\n",
    "        # https://stackoverflow.com/questions/31976273/open-s3-object-as-a-string-with-boto3\n",
    "        obj_body = json.loads(obj.get()['Body'].read().decode('utf-8'))\n",
    "        data_dict[obj_name] = obj_body\n",
    "    \n",
    "    # parse data into actual vehicle locations and errors\n",
    "    \n",
    "    print(f\"------- parsing data at {pendulum.now().to_datetime_string()}\")\n",
    "\n",
    "    data = pd.DataFrame()\n",
    "    errors = pd.DataFrame()\n",
    "\n",
    "    # k, v here are filename: full dict of JSON\n",
    "    for k, v in data_dict.items():\n",
    "        # print(f\"processing {k}\")\n",
    "        filename = k\n",
    "        new_data = pd.DataFrame()\n",
    "        new_errors = pd.DataFrame()\n",
    "        # expect ~12 \"chunks\" per JSON\n",
    "        for chunk, contents in v.items():\n",
    "            if 'vehicle' in v[chunk]['bustime-response'].keys():\n",
    "                new_data = new_data.append(pd.DataFrame(v[chunk]['bustime-response']['vehicle']))\n",
    "            if 'error' in v[chunk]['bustime-response'].keys():\n",
    "                new_errors = new_errors.append(pd.DataFrame(v[chunk]['bustime-response']['error']))\n",
    "        new_data['scrape_file'] = filename\n",
    "        new_errors['scrape_file'] = filename\n",
    "        data = data.append(new_data)\n",
    "        errors = errors.append(new_errors)\n",
    "        \n",
    "    print(f\"------- saving data at {pendulum.now().to_datetime_string()}\")\n",
    "\n",
    "    if len(errors) > 0:\n",
    "        bucket.put_object(Body = errors.to_csv(index = False), \n",
    "                   Key = f'bus_full_day_errors_v2/{date_str}.csv')\n",
    "\n",
    "    if len(data) > 0:\n",
    "        # convert data time to actual datetime\n",
    "        data['data_time'] = pd.to_datetime(data['tmstmp'], format='%Y%m%d %H:%M')\n",
    "\n",
    "        data['data_hour'] = data.data_time.dt.hour\n",
    "        data['data_date'] = data.data_time.dt.date\n",
    "\n",
    "        bucket.put_object(Body = data.to_csv(index = False), \n",
    "                   Key = f'bus_full_day_data_v2/{date_str}.csv')\n",
    "\n",
    "        # combine vids into a set (drops duplicates): https://stackoverflow.com/a/45925961\n",
    "        hourly_summary = data.groupby(['data_date', 'data_hour', 'rt', 'des']).agg({'vid': set, 'tatripid': set, 'tablockid': set}).reset_index()\n",
    "        # get number of vehicles per hour per route\n",
    "        hourly_summary['vh_count'] = hourly_summary['vid'].apply(len)\n",
    "        hourly_summary['trip_count'] = hourly_summary['tatripid'].apply(len)\n",
    "        hourly_summary['block_count'] = hourly_summary['tablockid'].apply(len)\n",
    "\n",
    "        bucket.put_object(Body = hourly_summary.to_csv(index = False), \n",
    "                   Key = f'bus_hourly_summary_v2/{date_str}.csv')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28358cb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a19126",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c95223",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0e6556",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6bb6af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d9cd77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21706de9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base_conda",
   "language": "python",
   "name": "base_conda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
